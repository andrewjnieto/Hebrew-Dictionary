{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing Hebrew Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook iterates over a series of dicitionaries found through open source repositories to create a singular, cohesize reference for words found in the Torah including the following information:\n",
    "\n",
    "* A variety of word definitions from different souces such as:\n",
    "    1. Strong's Concordance of the KJV\n",
    "        1. Unformatted strong's json -> https://raw.githubusercontent.com/openscriptures/strongs/master/hebrew/strongs-hebrew-dictionary.js\n",
    "        2. Strong's Concordance XML with extra definitions -> https://raw.githubusercontent.com/openscriptures/strongs/master/hebrew/StrongHebrewG.xml\n",
    "    2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the important libraries useful for the following work that will be done here.\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import pprint\n",
    "from statistics import median\n",
    "import xml.etree.ElementTree as ET\n",
    "from bidi.algorithm import get_display\n",
    "import sys\n",
    "from dynamodb_json import json_util as dynamo\n",
    "from boto3.dynamodb.types import TypeSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = open(\"definitions/unformatted_strongs.json\", \"r\", encoding=\"utf-8\")\n",
    "fileOut = open(\"definitions/added_non_nikud_defs.json\", \"w\", encoding=\"utf-8\")\n",
    "for line in fileIn:\n",
    "    hebID = line[:line.index(':')]\n",
    "    nikud_word = line[line.index('\"lemma\"') + 9 : line.index('\",\"xlit\"')]\n",
    "    non_nikud_word = \"\".join([c for c in unicodedata.normalize('NFKD', nikud_word) if not unicodedata.combining(c)])\n",
    "    new_def = f'{{ \"hebID\" : {hebID}, \"no_nikud\" : \"{non_nikud_word}\", '\n",
    "    new_def += line[line.index('\"lemma\"'):len(line) - 2] + \"\\n\"\n",
    "    fileOut.write(new_def)\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to include the morphology of the word in the definition of the word (i.e. the structure of the word [gender, plurality, etc.]). Unfortunately, that is in a different file, so we will have to splice it into the main definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = open('definitions/xml_defs.xml', 'r', encoding=\"utf-8\")\n",
    "morph_list = []\n",
    "for line in fileIn:\n",
    "    if('morph=' in line):\n",
    "        morph = line[line.index('morph=') + 6 : line.index(' POS=')]\n",
    "        morph_list.append(morph)\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the default definitions in strongs reference are good, they can be verbose and obscure. There are better definitions in the file \"xml_defs.xml\", so we will take those and add them to our default definitions. We will start be adding all the definitions to a list to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_defs = []\n",
    "fileIn = open(\"definitions/xml_defs.xml\", 'r', encoding=\"utf-8\")\n",
    "line = fileIn.readline()\n",
    "index = 1\n",
    "while line:\n",
    "    #If we come across a list of definitions\n",
    "    if '<list>' in line:\n",
    "        defs = \"[\"\n",
    "        line = fileIn.readline()\n",
    "        #While there are still more definitions to be read\n",
    "        while '</list>' not in line:\n",
    "            line = line.strip()\n",
    "            startTagIndex = line.index('<item>')\n",
    "            endTagIndex = line.index('</item>')\n",
    "            #Escape the quotation marks within the line.\n",
    "            line = line[startTagIndex + 6 : endTagIndex].replace('\"', '\\\\\"')\n",
    "            defs += f'\"{line}\", '\n",
    "            line = fileIn.readline()\n",
    "        defs = defs[:len(defs) - 2] + \"]\"\n",
    "        all_defs.append(defs)\n",
    "        defs = []\n",
    "        index += 1\n",
    "    line = fileIn.readline()\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the definitions we have acquired along with the morphology of the word and insert it into the definition along with all other previous definition information.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to insert the morphology into the definitions line.\n",
    "def insertMorph(main_line, morph):\n",
    "    xlit_index = main_line.index('\"xlit\":')\n",
    "    return main_line[:xlit_index] + f' \"morph\":{morph}, ' + main_line[xlit_index:]\n",
    "\n",
    "#Used to insert the additional defs into the definitions line.\n",
    "def insertDefs(main_line, defs):\n",
    "    defs = f', \"all_defs\" : {defs}}}\\n'\n",
    "    return main_line[:len(main_line) - 2] + defs\n",
    "\n",
    "fileIn = open(\"definitions/added_non_nikud_defs.json\", \"r\", encoding = \"utf-8\")\n",
    "fileOut = open(\"definitions/added_more_defs.json\", \"w\", encoding = \"utf-8\")\n",
    "for i in range(8674):\n",
    "    main_line = fileIn.readline()\n",
    "    main_line = insertMorph(main_line,morph_list[i])\n",
    "    main_line = insertDefs(main_line, all_defs[i])\n",
    "    main_line = main_line.replace(\"nikud\", \"niqqud\", 1)\n",
    "    main_line = main_line.replace(\"lemma\", \"niqqud\", 1)\n",
    "    fileOut.write(main_line)\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though quite inefficient now that I look back on it, we will take all the new definitions that we have put into the file added_more_defs and now add them them to a dictionary that we will use to process the definitions into a more manageable format given that there are many sub-definitions that are labeled using an inefficient system of numbers and letters as opposed to one of depths (i.e. top definition then sub-definition then another sub-definition and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_defs = []\n",
    "fileIn = open(\"definitions/added_more_defs.json\", 'r', encoding = 'utf-8')\n",
    "for word in fileIn:\n",
    "    all_defs_index = word.index('\"all_defs\"')\n",
    "    dictionary_defs.append(json.loads(\"{\" + word[all_defs_index:]))\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following then takes in all the hebrew words and dictionary definitions and imports them into a dictionary that will process them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does several things in order to clean up all the extra definitions that we will be adding to our words.\n",
    "The first thing that the code will do is that for each of the word's definitions, the definitions will remove the obsolete and single depth ordering method in favor of one that utilizes JSON objects and lists to differentiate the definitions and the sub definitions.\n",
    "\n",
    "The first part is not entirely perfect, and leaves many empty definitions in addition to the proper ones, we will utilize a flattening algorithm that will take the definitions and remove all the empty definitions and leave only the original definitions. We are utilizing this method instead of trying to sort out the problem with the main organization algorithm because we will only run this once and the organization algorithm is not corrupting the original definitions which is the most important part.\n",
    "\n",
    "The third part will remove the inefficient numbering still remaining and the fourth part will convert the format of definitions being contained in lists into dictionaries in order to have them be easily stored as json objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Organize the defs of each word according to their depth\n",
    "#relative to a parent/ancestor.\n",
    "def organize_defs(defs):\n",
    "    results = []\n",
    "    #if a parentheses could not be found, make the\n",
    "    #base depth 1 (i.e. Abi = \"my father\" (H21))\n",
    "    try:\n",
    "        cur_depth = len(defs[0][:defs[0].index(\")\")])\n",
    "    except:\n",
    "        cur_depth = 1\n",
    "    index = 0\n",
    "    #for each definition in the list\n",
    "    while(index != len(defs)):\n",
    "        #if a parenthesis could not be found in the def\n",
    "        #simply append it to the results list and continue.\n",
    "        try:\n",
    "            def_depth = len(defs[index][:defs[index].index(\")\")])\n",
    "        except:\n",
    "            results.append(defs[index])\n",
    "            index += 1\n",
    "            continue\n",
    "        #Prevents words with non-essential parentheses from having a\n",
    "        #greater depth than they should. 5 was chosen because this type\n",
    "        #of thing occured more when there was 5 or more depths to a word's defintions.\n",
    "        if(def_depth > 5 and '=' in defs[index]):\n",
    "            results.append(defs[index])\n",
    "            index += 1\n",
    "            continue\n",
    "        #if on the same level, simply append.\n",
    "        if(def_depth == cur_depth):\n",
    "            results.append(defs[index])\n",
    "            index += 1\n",
    "        elif(def_depth > cur_depth):\n",
    "            #if the depth has increased, call the function again on\n",
    "            #every def after that one, inclusive. Append the resulting list.\n",
    "            sub_list = organize_defs(defs[index:])\n",
    "            results.append(sub_list[1])\n",
    "            #if the functions have reached the end of all defs.\n",
    "            if(sub_list[0] == -1):\n",
    "                return (-1, results)\n",
    "            #move up to where the recursive call left off.\n",
    "            index += sub_list[0]\n",
    "        else:\n",
    "            #we return the index of where the recursive call ended\n",
    "            #and the sublist to be appended.\n",
    "            return (index, results)\n",
    "    #return this if reached the end of the defs.\n",
    "    return (-1, results)\n",
    "\n",
    "#flattens the defs list to not include numbers\n",
    "def flatten(defs):\n",
    "    if(type(defs[0]) is str):\n",
    "        return defs\n",
    "    if(type(defs[0]) is int):\n",
    "        return flatten(defs[1])\n",
    "    return defs\n",
    "\n",
    "#Removes the numbering from the definitions\n",
    "def removeNumbering(defs):\n",
    "    index = 0\n",
    "    while(index != len(defs)):\n",
    "        if(type(defs[index]) == list):\n",
    "            defs[index] = removeNumbering(defs[index])\n",
    "        else:\n",
    "            try:\n",
    "                p_index = defs[index].index(\")\")\n",
    "            except:\n",
    "                index += 1\n",
    "                continue\n",
    "            defs[index] = defs[index][p_index + 1:].strip()\n",
    "        index += 1\n",
    "    return defs\n",
    "\n",
    "#Transform the the nesting of the definitions from lists into dictionaries.\n",
    "def changeNestedTypes(defs):\n",
    "    results = []\n",
    "    index = 0\n",
    "    for definition in defs:\n",
    "        if(type(definition) is str):\n",
    "            results.append({\"definition\" : definition})\n",
    "        elif(type(definition) is list):\n",
    "            sub_results = changeNestedTypes(definition)\n",
    "            results.append({\"senses\" : sub_results})\n",
    "        index += 1\n",
    "    return results\n",
    "\n",
    "#Organize all the definitions, flatten and remove\n",
    "#numbering from the results\n",
    "for i, word in enumerate(dictionary_defs):\n",
    "    organized_defs = organize_defs(word['all_defs'])\n",
    "    flattened_defs = flatten(organized_defs)\n",
    "    unnumbered_defs = removeNumbering(flattened_defs)\n",
    "    word['all_defs'] = changeNestedTypes(unnumbered_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last part will now replace the old definitions format with the new and improved one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = open(\"definitions/added_more_defs.json\", 'r', encoding = 'utf-8')\n",
    "fileOut = open(\"definitions/final_defs.json\", \"w\", encoding = 'utf-8')\n",
    "for index, word in enumerate(fileIn):\n",
    "    all_defs_index = word.index('\"all_defs\" : ')\n",
    "    word_defs = json.dumps(dictionary_defs[index], ensure_ascii=False)\n",
    "    fileOut.write(word[:all_defs_index] + word_defs[1:len(word_defs) - 1] + \"}\\n\")\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/7/2021: A problem with the current definitions is that when they are in the db and a call is made to retrieve the definitions matching the word \"mother\", there is no ranking system to give more weight to certain words. So definitions that are a match, but whose frequency in the bible is limited can occur way before the most frequent match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a new file containing the words along with the number of references the word has to calculate the frequency of the word and improve the results returned by calls to the hebrew db.\n",
    "\n",
    "The first task will be to organize the words since they are in hebraic-alphabetic order as opposed to being order by strong's concordance ids, which is useful for comparing our previous output in the last cell to the one here to ensure that the data isn't wonky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = open('definitions/words_refs.xml', 'r', encoding='utf-8')\n",
    "line = fileIn.readline()\n",
    "sorted_words = []\n",
    "while line:\n",
    "    if \"<w\" in line:\n",
    "        #Corresponds to strongs concordance number.\n",
    "        key_value = line[line.index('a=\"') + 3:line.rindex(\"\\\"\")]\n",
    "        key_value = int(re.sub(\"[^0-9]\", \"\", key_value))\n",
    "        complete_line = \"\"\n",
    "        while \"</w>\" not in line:\n",
    "            complete_line += line\n",
    "            line = fileIn.readline()\n",
    "        sorted_words.append((key_value, complete_line))\n",
    "    line = fileIn.readline()\n",
    "sorted_words = sorted(sorted_words)\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with the words that we have organized is that the same word may be registered as many different words with many references, when we just need them organized by their primary form like is done in strong's concordance and thus in the previous output's we have done. We will thus consolidate the words into their main forms to make it easy to transfer the information into the document we have previously completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_words = {}\n",
    "for word in sorted_words:\n",
    "    if word[0] in aggregated_words:\n",
    "        cur_refs = aggregated_words[word[0]]\n",
    "        cur_refs += word[1]\n",
    "        aggregated_words[word[0]] = cur_refs\n",
    "    else:\n",
    "        aggregated_words[word[0]] = word[1]\n",
    "#Inefficiency exists here in that we repeat a step two times. Probably can be improved, but computes fast, so will stay.\n",
    "for word in sorted(aggregated_words.items()):\n",
    "    aggregated_words[word[0]] = word[1].count('<r')\n",
    "aggregated_words = sorted(aggregated_words.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now take the words and insert the frequency into the main document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileIn = open('definitions/final_defs.json', 'r', encoding='utf-8')\n",
    "fileOut = open('definitions/final_defs_v2.json', 'w', encoding='utf-8')\n",
    "#The median frequency of all the words\n",
    "median_frequency = int(median([num[1] for num in aggregated_words]))\n",
    "for index, line in enumerate(fileIn):\n",
    "    #Some words have no frequency apparently. If that happens, use the median frequency instead to give them some weight.\n",
    "    try:\n",
    "        line = line[:len(line) - 2] + f', \"frequency\" : {aggregated_words[index + 1][1]}}}\\n'\n",
    "    except IndexError:\n",
    "        line = line[:len(line) - 2] + f', \"frequency\" : {median_frequency} }}\\n'\n",
    "    fileOut.write(line)\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to find out how many references each word has in the document called \"sorted_words_refs.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_defs = {}\n",
    "book_mapping = json.loads('{\"Gen\":1,\"Exod\":2,\"Lev\":3,\"Num\":4,\"Deut\":5,\"Josh\":6,\"Judg\":7,\"1Sam\":8,\"2Sam\":9,\"1Kgs\":10,\"2Kgs\":11,\"Isa\":12,\"Jer\":13,\"Ezek\":14,\"Hos\":15,\"Joel\":16,\"Amos\":17,\"Obad\":18,\"Jonah\":19,\"Mic\":20,\"Nah\":21,\"Hab\":22,\"Zeph\":23,\"Hag\":24,\"Zech\":25,\"Mal\":26,\"Ps\":27,\"Prov\":28,\"Job\":29,\"Song\":30,\"Ruth\":31,\"Lam\":32,\"Eccl\":33,\"Esth\":34,\"Dan\":35,\"Ezra\":36,\"Neh\":37,\"1Chr\":38,\"2Chr\":39}')\n",
    "with open(\"./definitions/final_defs_v2.json\", \"r\", encoding=\"utf-8\") as previous_defs:\n",
    "    for index, definition in enumerate(previous_defs):\n",
    "        try:\n",
    "            cur_def = json.loads(definition)\n",
    "            cur_def['deprecated_freq'] = cur_def['frequency']\n",
    "            del cur_def['frequency']\n",
    "            cur_def['total_freq'] = 0\n",
    "            cur_def['variants_with_refs'] = []\n",
    "            incomplete_defs[str(index + 1)] = cur_def\n",
    "        except ValueError:\n",
    "            print(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "direction": "ltr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"./definitions/sorted_words_refs.xml\", \"r\", encoding=\"utf-8\") as sorted_words:\n",
    "    cur_hid = 0\n",
    "    cur_niqqud = ''\n",
    "    cur_non_niqqud = ''\n",
    "    cur_ref_dict = {}\n",
    "    cur_total_freq = 0\n",
    "    for i, line in enumerate(sorted_words):\n",
    "        trim_line = line.strip()\n",
    "        if trim_line[1] == 'w':\n",
    "            if cur_hid in incomplete_defs:\n",
    "                definition = incomplete_defs[cur_hid]\n",
    "                def_to_add = {'niqqud' : cur_niqqud,\n",
    "                              'non_niqqud' : cur_non_niqqud,\n",
    "                              'references' : cur_ref_dict}\n",
    "                definition['variants_with_refs'].append(def_to_add)\n",
    "                definition['total_freq'] += cur_total_freq\n",
    "                cur_ref_dict = {}\n",
    "                cur_niqqud = ''\n",
    "                cur_non_niqqud = ''\n",
    "                cur_total_freq = 0\n",
    "            trim_line = trim_line[1: len(trim_line) - 1]\n",
    "            word_parts = [part[3:len(part) - 1] for part in trim_line.split()[1:]]\n",
    "            cur_non_niqqud, cur_niqqud, cur_hid = word_parts\n",
    "            if not cur_hid[-1].isdigit():\n",
    "                cur_hid = cur_hid[:len(cur_hid) - 1]\n",
    "        else:\n",
    "            lst_rght_crt = trim_line[:-1].rindex('>')\n",
    "            lst_lft_crt = trim_line[:-1].rindex('<')\n",
    "            book, chapter, verse = trim_line[:-1][lst_rght_crt + 1:lst_lft_crt].split('.')\n",
    "            if book not in cur_ref_dict:\n",
    "                cur_ref_dict[book] = {chapter : {verse : 1}}\n",
    "            elif chapter not in cur_ref_dict[book]:\n",
    "                cur_ref_dict[book][chapter] = {verse : 1}\n",
    "            elif verse not in cur_ref_dict[book][chapter]:\n",
    "                cur_ref_dict[book][chapter][verse] = 1\n",
    "            else:\n",
    "                cur_ref_dict[book][chapter][verse] += 1\n",
    "            cur_total_freq += 1\n",
    "#Handles the last word\n",
    "if cur_hid in incomplete_defs:\n",
    "    definition = incomplete_defs[cur_hid]\n",
    "    def_to_add = {'niqqud' : cur_niqqud,\n",
    "                  'non_niqqud' : cur_non_niqqud,\n",
    "                  'references' : cur_ref_dict}\n",
    "    definition['variants_with_refs'].append(def_to_add)\n",
    "    definition['total_freq'] += cur_total_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./definitions/newest.json', 'w', encoding='utf-8') as variants:\n",
    "    for item in incomplete_defs.values():\n",
    "        item[\"hebID\"] = int(item['hebID'][1:])\n",
    "        del item[\"deprecated_freq\"]\n",
    "        #variants.write(json.dumps({k: serializer.serialize(v) for k, v in item.items()}, ensure_ascii=False) + '\\n')\n",
    "        variants.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "my_config = Config(\n",
    "    region_name = '',\n",
    "    signature_version = 'v4',\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")\n",
    "\n",
    "client = boto3.client('dynamodb',\n",
    "                      config=my_config,\n",
    "                      aws_access_key_id='',\n",
    "                      aws_secret_access_key='')\n",
    "for item in incomplete_defs.values():\n",
    "    item_to_put = {k: serializer.serialize(v) for k, v in item.items()}\n",
    "    client.put_item(TableName='hebrew_dict',\n",
    "                   Item=item_to_put\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
