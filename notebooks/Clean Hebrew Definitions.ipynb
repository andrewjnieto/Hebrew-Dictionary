{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesizing Hebrew Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that much of this notebook was written using outdated methodologies and processes that should not be followed. This notebook is published simply for procedural and historical purposes.\n",
    "\n",
    "A task that could be completed in the future is the refactoring of this notebook such that it is simpler and relies less of string parsing and more concise, idiomatic methodologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook iterates over a series of dicitionaries found through open source repositories to create a singular, cohesize reference for words found in the Torah including the following information\n",
    "\n",
    "Please see readme.md for a full explanation and citation of all the relevant sources used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:36.600474Z",
     "start_time": "2023-02-22T04:41:36.482475Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing the important libraries useful for the following work that will be done here.\n",
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "import pprint\n",
    "from statistics import mean, median\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring some of the definitions and shifting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:37.824331Z",
     "start_time": "2023-02-22T04:41:37.769331Z"
    }
   },
   "outputs": [],
   "source": [
    "fileIn = open(\"../definitions/input/unformatted_strongs.json\", \"r\", encoding=\"utf-8\")\n",
    "fileOut = open(\"../definitions/input/added_non_nikud_defs.json\", \"w\", encoding=\"utf-8\")\n",
    "for line in fileIn:\n",
    "    hebID = line[:line.index(':')]\n",
    "    nikud_word = line[line.index('\"lemma\"') + 9 : line.index('\",\"xlit\"')]\n",
    "    non_nikud_word = \"\".join([c for c in unicodedata.normalize('NFKD', nikud_word) if not unicodedata.combining(c)])\n",
    "    new_def = f'{{ \"hebID\" : {hebID}, \"no_nikud\" : \"{non_nikud_word}\", '\n",
    "    new_def += line[line.index('\"lemma\"'):len(line) - 2] + \"\\n\"\n",
    "    fileOut.write(new_def)\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to include the morphology of the word in the definition of the word. Importing from different file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:38.582351Z",
     "start_time": "2023-02-22T04:41:38.541349Z"
    }
   },
   "outputs": [],
   "source": [
    "fileIn = open('../definitions/input/xml_defs.xml', 'r', encoding=\"utf-8\")\n",
    "morph_list = []\n",
    "for line in fileIn:\n",
    "    if('morph=' in line):\n",
    "        morph = line[line.index('morph=') + 6 : line.index(' POS=')]\n",
    "        morph_list.append(morph)\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding open source definitions that are less verbose and historical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:39.193163Z",
     "start_time": "2023-02-22T04:41:39.113156Z"
    }
   },
   "outputs": [],
   "source": [
    "all_defs = []\n",
    "fileIn = open(\"../definitions/input/xml_defs.xml\", 'r', encoding=\"utf-8\")\n",
    "line = fileIn.readline()\n",
    "index = 1\n",
    "while line:\n",
    "    #If we come across a list of definitions\n",
    "    if '<list>' in line:\n",
    "        defs = \"[\"\n",
    "        line = fileIn.readline()\n",
    "        #While there are still more definitions to be read\n",
    "        while '</list>' not in line:\n",
    "            line = line.strip()\n",
    "            startTagIndex = line.index('<item>')\n",
    "            endTagIndex = line.index('</item>')\n",
    "            #Escape the quotation marks within the line.\n",
    "            line = line[startTagIndex + 6 : endTagIndex].replace('\"', '\\\\\"')\n",
    "            defs += f'\"{line}\", '\n",
    "            line = fileIn.readline()\n",
    "        defs = defs[:len(defs) - 2] + \"]\"\n",
    "        all_defs.append(defs)\n",
    "        defs = []\n",
    "        index += 1\n",
    "    line = fileIn.readline()\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting morph and open source definitions into the main definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:39.755981Z",
     "start_time": "2023-02-22T04:41:39.597474Z"
    }
   },
   "outputs": [],
   "source": [
    "#Used to insert the morphology into the definitions line.\n",
    "def insertMorph(main_line, morph):\n",
    "    xlit_index = main_line.index('\"xlit\":')\n",
    "    return main_line[:xlit_index] + f' \"morph\":{morph}, ' + main_line[xlit_index:]\n",
    "\n",
    "#Used to insert the additional defs into the definitions line.\n",
    "def insertDefs(main_line, defs):\n",
    "    defs = f', \"all_defs\" : {defs}}}\\n'\n",
    "    return main_line[:len(main_line) - 2] + defs\n",
    "\n",
    "fileIn = open(\"../definitions/input/added_non_nikud_defs.json\", \"r\", encoding = \"utf-8\")\n",
    "fileOut = open(\"../definitions/input/added_more_defs.json\", \"w\", encoding = \"utf-8\")\n",
    "for i in range(8674):\n",
    "    main_line = fileIn.readline()\n",
    "    main_line = insertMorph(main_line,morph_list[i])\n",
    "    main_line = insertDefs(main_line, all_defs[i])\n",
    "    main_line = main_line.replace(\"nikud\", \"niqqud\", 1)\n",
    "    main_line = main_line.replace(\"lemma\", \"niqqud\", 1)\n",
    "    fileOut.write(main_line)\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:40.445330Z",
     "start_time": "2023-02-22T04:41:40.270330Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary_defs = []\n",
    "fileIn = open(\"../definitions/input/added_more_defs.json\", 'r', encoding = 'utf-8')\n",
    "for word in fileIn:\n",
    "    all_defs_index = word.index('\"all_defs\"')\n",
    "    dictionary_defs.append(json.loads(\"{\" + word[all_defs_index:]))\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following then takes in all the hebrew words and dictionary definitions and imports them into a dictionary that will process them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does several things in order to clean up all the extra definitions that we will be adding to our words.\n",
    "\n",
    "First the definitions will remove the obsolete and single depth ordering method in favor of one that utilizes JSON objects and lists to differentiate the definitions and the sub definitions.\n",
    "\n",
    "Second part will flatten the definitions and remove all the empty definitions.\n",
    "\n",
    "The third part will remove the old numbering\n",
    "\n",
    "Fourth part will convert the format of definitions being contained in lists into dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:41.523680Z",
     "start_time": "2023-02-22T04:41:41.442668Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Organize the defs of each word according to their depth\n",
    "#relative to a parent/ancestor.\n",
    "def organize_defs(defs):\n",
    "    results = []\n",
    "    #if a parentheses could not be found, make the\n",
    "    #base depth 1 (i.e. Abi = \"my father\" (H21))\n",
    "    try:\n",
    "        cur_depth = len(defs[0][:defs[0].index(\")\")])\n",
    "    except:\n",
    "        cur_depth = 1\n",
    "    index = 0\n",
    "    #for each definition in the list\n",
    "    while(index != len(defs)):\n",
    "        #if a parenthesis could not be found in the def\n",
    "        #simply append it to the results list and continue.\n",
    "        try:\n",
    "            def_depth = len(defs[index][:defs[index].index(\")\")])\n",
    "        except:\n",
    "            results.append(defs[index])\n",
    "            index += 1\n",
    "            continue\n",
    "        #Prevents words with non-essential parentheses from having a\n",
    "        #greater depth than they should. 5 was chosen because this type\n",
    "        #of thing occured more when there was 5 or more depths to a word's defintions.\n",
    "        if(def_depth > 5 and '=' in defs[index]):\n",
    "            results.append(defs[index])\n",
    "            index += 1\n",
    "            continue\n",
    "        #if on the same level, simply append.\n",
    "        if(def_depth == cur_depth):\n",
    "            results.append(defs[index])\n",
    "            index += 1\n",
    "        elif(def_depth > cur_depth):\n",
    "            #if the depth has increased, call the function again on\n",
    "            #every def after that one, inclusive. Append the resulting list.\n",
    "            sub_list = organize_defs(defs[index:])\n",
    "            results.append(sub_list[1])\n",
    "            #if the functions have reached the end of all defs.\n",
    "            if(sub_list[0] == -1):\n",
    "                return (-1, results)\n",
    "            #move up to where the recursive call left off.\n",
    "            index += sub_list[0]\n",
    "        else:\n",
    "            #we return the index of where the recursive call ended\n",
    "            #and the sublist to be appended.\n",
    "            return (index, results)\n",
    "    #return this if reached the end of the defs.\n",
    "    return (-1, results)\n",
    "\n",
    "#flattens the defs list to not include numbers\n",
    "def flatten(defs):\n",
    "    if(type(defs[0]) is str):\n",
    "        return defs\n",
    "    if(type(defs[0]) is int):\n",
    "        return flatten(defs[1])\n",
    "    return defs\n",
    "\n",
    "#Removes the numbering from the definitions\n",
    "def removeNumbering(defs):\n",
    "    index = 0\n",
    "    while(index != len(defs)):\n",
    "        if(type(defs[index]) == list):\n",
    "            defs[index] = removeNumbering(defs[index])\n",
    "        else:\n",
    "            try:\n",
    "                p_index = defs[index].index(\")\")\n",
    "            except:\n",
    "                index += 1\n",
    "                continue\n",
    "            defs[index] = defs[index][p_index + 1:].strip()\n",
    "        index += 1\n",
    "    return defs\n",
    "\n",
    "#Transform the the nesting of the definitions from lists into dictionaries.\n",
    "def changeNestedTypes(defs):\n",
    "    results = []\n",
    "    index = 0\n",
    "    for definition in defs:\n",
    "        if(type(definition) is str):\n",
    "            results.append({\"def\" : definition})\n",
    "        elif(type(definition) is list):\n",
    "            sub_results = changeNestedTypes(definition)\n",
    "            results.append({\"senses\" : sub_results})\n",
    "        index += 1\n",
    "    return results\n",
    "\n",
    "#Organize all the definitions, flatten and remove\n",
    "#numbering from the results\n",
    "for i, word in enumerate(dictionary_defs):\n",
    "    #Assume this follows a first, second, third, and fourth part\n",
    "    organized_defs = organize_defs(word['all_defs'])\n",
    "    flattened_defs = flatten(organized_defs)\n",
    "    unnumbered_defs = removeNumbering(flattened_defs)\n",
    "    word['all_defs'] = changeNestedTypes(unnumbered_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last part will now replace the old definitions format with the new and improved one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:42.600785Z",
     "start_time": "2023-02-22T04:41:42.491787Z"
    }
   },
   "outputs": [],
   "source": [
    "fileIn = open(\"../definitions/input/added_more_defs.json\", 'r', encoding = 'utf-8')\n",
    "fileOut = open(\"../definitions/input/final_defs.json\", \"w\", encoding = 'utf-8')\n",
    "for index, word in enumerate(fileIn):\n",
    "    all_defs_index = word.index('\"all_defs\" : ')\n",
    "    word_defs = json.dumps(dictionary_defs[index], ensure_ascii=False)\n",
    "    fileOut.write(word[:all_defs_index] + word_defs[1:len(word_defs) - 1] + \"}\\n\")\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a new file containing the words along with the number of references the word has to calculate the frequency of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:44.190153Z",
     "start_time": "2023-02-22T04:41:43.879154Z"
    }
   },
   "outputs": [],
   "source": [
    "fileIn = open('../definitions/input/words_refs.xml', 'r', encoding='utf-8')\n",
    "line = fileIn.readline()\n",
    "sorted_words = []\n",
    "while line:\n",
    "    if \"<w\" in line:\n",
    "        #Corresponds to strongs concordance number.\n",
    "        key_value = line[line.index('a=\"') + 3:line.rindex(\"\\\"\")]\n",
    "        key_value = int(re.sub(\"[^0-9]\", \"\", key_value))\n",
    "        complete_line = \"\"\n",
    "        while \"</w>\" not in line:\n",
    "            complete_line += line\n",
    "            line = fileIn.readline()\n",
    "        sorted_words.append((key_value, complete_line))\n",
    "    line = fileIn.readline()\n",
    "sorted_words = sorted(sorted_words)\n",
    "fileIn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with the words that we have organized is that the same word may be registered as many different words with many references, when we just need them organized by their primary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:46.217138Z",
     "start_time": "2023-02-22T04:41:46.152139Z"
    }
   },
   "outputs": [],
   "source": [
    "aggregated_words = {}\n",
    "for word in sorted_words:\n",
    "    if word[0] in aggregated_words:\n",
    "        cur_refs = aggregated_words[word[0]]\n",
    "        cur_refs += word[1]\n",
    "        aggregated_words[word[0]] = cur_refs\n",
    "    else:\n",
    "        aggregated_words[word[0]] = word[1]\n",
    "for word in sorted(aggregated_words.items()):\n",
    "    aggregated_words[word[0]] = word[1].count('<r')\n",
    "aggregated_words = sorted(aggregated_words.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the frequency into the main document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:41:51.844118Z",
     "start_time": "2023-02-22T04:41:51.661119Z"
    }
   },
   "outputs": [],
   "source": [
    "fileIn = open('../definitions/input/final_defs.json', 'r', encoding='utf-8')\n",
    "fileOut = open('../definitions/input/final_defs_v2.json', 'w', encoding='utf-8')\n",
    "#The median frequency of all the words\n",
    "median_frequency = int(median([num[1] for num in aggregated_words]))\n",
    "for index, line in enumerate(fileIn):\n",
    "    #Some words have no frequency apparently. If that happens, use the median frequency instead to give them some weight.\n",
    "    try:\n",
    "        line = line[:len(line) - 2] + f', \"frequency\" : {aggregated_words[index + 1][1]}}}\\n'\n",
    "    except IndexError:\n",
    "        line = line[:len(line) - 2] + f', \"frequency\" : {median_frequency} }}\\n'\n",
    "    fileOut.write(line)\n",
    "fileIn.close()\n",
    "fileOut.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T18:41:17.877304Z",
     "start_time": "2023-02-21T18:41:17.873304Z"
    }
   },
   "source": [
    "Updating the frequency (old one is inaccurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:42:19.941859Z",
     "start_time": "2023-02-22T04:42:19.812860Z"
    }
   },
   "outputs": [],
   "source": [
    "incomplete_defs = {}\n",
    "book_mapping = json.loads('{\"Gen\":1,\"Exod\":2,\"Lev\":3,\"Num\":4,\"Deut\":5,\"Josh\":6,\"Judg\":7,\"1Sam\":8,\"2Sam\":9,\"1Kgs\":10,\"2Kgs\":11,\"Isa\":12,\"Jer\":13,\"Ezek\":14,\"Hos\":15,\"Joel\":16,\"Amos\":17,\"Obad\":18,\"Jonah\":19,\"Mic\":20,\"Nah\":21,\"Hab\":22,\"Zeph\":23,\"Hag\":24,\"Zech\":25,\"Mal\":26,\"Ps\":27,\"Prov\":28,\"Job\":29,\"Song\":30,\"Ruth\":31,\"Lam\":32,\"Eccl\":33,\"Esth\":34,\"Dan\":35,\"Ezra\":36,\"Neh\":37,\"1Chr\":38,\"2Chr\":39}')\n",
    "with open(\"../definitions/input/final_defs_v2.json\", \"r\", encoding=\"utf-8\") as previous_defs:\n",
    "    for index, definition in enumerate(previous_defs):\n",
    "        try:\n",
    "            cur_def = json.loads(definition)\n",
    "            cur_def['deprecated_freq'] = cur_def['frequency']\n",
    "            del cur_def['frequency']\n",
    "            cur_def['total_freq'] = 0\n",
    "            cur_def['variants'] = []\n",
    "            incomplete_defs[str(index + 1)] = cur_def\n",
    "        except ValueError:\n",
    "            print(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T18:41:35.021264Z",
     "start_time": "2023-02-21T18:41:35.002263Z"
    }
   },
   "source": [
    "Adding references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:42:22.083728Z",
     "start_time": "2023-02-22T04:42:21.036740Z"
    },
    "direction": "ltr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../definitions/input/sorted_words_refs.xml\", \"r\", encoding=\"utf-8\") as sorted_words:\n",
    "    cur_hid = 0\n",
    "    cur_niqqud = ''\n",
    "    cur_non_niqqud = ''\n",
    "    cur_ref_dict = {}\n",
    "    cur_total_freq = 0\n",
    "    for i, line in enumerate(sorted_words):\n",
    "        trim_line = line.strip()\n",
    "        if trim_line[1] == 'w':\n",
    "            if cur_hid in incomplete_defs:\n",
    "                definition = incomplete_defs[cur_hid]\n",
    "                def_to_add = {'niqqud' : cur_niqqud,\n",
    "                              'no_niqqud' : cur_non_niqqud,\n",
    "                              'refs' : cur_ref_dict}\n",
    "                definition['variants'].append(def_to_add)\n",
    "                definition['total_freq'] += cur_total_freq\n",
    "                cur_ref_dict = {}\n",
    "                cur_niqqud = ''\n",
    "                cur_non_niqqud = ''\n",
    "                cur_total_freq = 0\n",
    "            trim_line = trim_line[1: len(trim_line) - 1]\n",
    "            word_parts = [part[3:len(part) - 1] for part in trim_line.split()[1:]]\n",
    "            cur_non_niqqud, cur_niqqud, cur_hid = word_parts\n",
    "            if not cur_hid[-1].isdigit():\n",
    "                cur_hid = cur_hid[:len(cur_hid) - 1]\n",
    "        else:\n",
    "            lst_rght_crt = trim_line[:-1].rindex('>')\n",
    "            lst_lft_crt = trim_line[:-1].rindex('<')\n",
    "            book, chapter, verse = trim_line[:-1][lst_rght_crt + 1:lst_lft_crt].split('.')\n",
    "            if book not in cur_ref_dict:\n",
    "                cur_ref_dict[book] = {chapter : {verse : 1}}\n",
    "            elif chapter not in cur_ref_dict[book]:\n",
    "                cur_ref_dict[book][chapter] = {verse : 1}\n",
    "            elif verse not in cur_ref_dict[book][chapter]:\n",
    "                cur_ref_dict[book][chapter][verse] = 1\n",
    "            else:\n",
    "                cur_ref_dict[book][chapter][verse] += 1\n",
    "            cur_total_freq += 1\n",
    "#Handles the last word\n",
    "if cur_hid in incomplete_defs:\n",
    "    definition = incomplete_defs[cur_hid]\n",
    "    def_to_add = {'niqqud' : cur_niqqud,\n",
    "                  'no_niqqud' : cur_non_niqqud,\n",
    "                  'refs' : cur_ref_dict}\n",
    "    definition['variants'].append(def_to_add)\n",
    "    definition['total_freq'] += cur_total_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting final document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T04:42:23.263500Z",
     "start_time": "2023-02-22T04:42:22.880732Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../definitions/clean_defs.json', 'w', encoding='utf-8') as variants:\n",
    "    for item in incomplete_defs.values():\n",
    "        item[\"hebID\"] = int(item['hebID'][1:])\n",
    "        del item[\"deprecated_freq\"]\n",
    "        variants.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
